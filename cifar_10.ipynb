{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar-10",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNoD9sn0PDAzGkarWT5MURl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dibya-pati/CIFAR-10/blob/master/cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b_k4Fx_Gzk4",
        "colab_type": "code",
        "outputId": "6bd24f04-8fc0-4d66-b611-56fc29755a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 22 08:11:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIh49vEeHC3C",
        "colab_type": "code",
        "outputId": "bc80c724-467f-41ab-9169-e82536aeb3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5upyyPbIpu9",
        "colab_type": "code",
        "outputId": "89de2e11-49e3-494a-d2d7-bc767cceb9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "%tensorflow_version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa2N3gj3Iuri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC_nRlR7iI0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def CeLU(x):\n",
        "  alpha = tf.constant(0.075, dtype=tf.float32)\n",
        "  #return tf.cond(tf.math.greater_equal(x, 0), x, alpha*(tf.math.exp(x/alpha) - 1))\n",
        "  return tf.keras.activations.elu(x, alpha=0.075)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTH_xoY3I7VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same', dilation_rate=2))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same', dilation_rate=2))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(192, (3, 3), activation=CeLU, kernel_initializer='he_uniform',\\\n",
        "\t                 padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "\t#model.add(Flatten())\n",
        "\t#model.add(Dense(128, activation=CeLU, kernel_initializer='he_uniform'))\n",
        "\t#model.add(BatchNormalization())\n",
        "\t#model.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\treturn model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBYiRp0PJXAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmgzInqqLgbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augtrain(X,y):\n",
        "  X = tf.cast(X, tf.float16)\n",
        "  X = tf.pad(X, [[6,6], [6,6], [0,0]], mode=\"SYMMETRIC\")\n",
        "  X = tf.image.random_crop(X, [32, 32, 3])\n",
        "  X = tf.image.random_flip_left_right(X)\n",
        "  X = tf.image.random_brightness(X, 0.2)  \n",
        "\n",
        "  return X, y\n",
        "\n",
        "def augtest(X,y):\n",
        "  return tf.cast(X, tf.float16), y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VEWJKSSKxzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfdatagen(X, y, batchsize=256, epochs=50, train=True):\n",
        "  cif_mean = \\\n",
        "        np.array([0.4913997551666284, 0.48215855929893703, 0.4465309133731618])\n",
        "  cif_std = \\\n",
        "        np.array([0.24703225141799082, 0.24348516474564, 0.26158783926049628])\n",
        "\n",
        "  X = X.astype(np.float32)/255.0\n",
        "  y = y.astype(np.float32)\n",
        "\n",
        "  X = (X - cif_mean)/cif_std\n",
        "\n",
        "  y = to_categorical(y)\n",
        "  \n",
        "  if train:\n",
        "    y = np.where(y==1, 0.8, 0.2/9)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X,y)).repeat(epochs).\\\n",
        "              shuffle(buffer_size=10*batchsize).\\\n",
        "              map(augtrain, num_parallel_calls=tf.data.experimental.AUTOTUNE).\\\n",
        "              batch(batchsize).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  else:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X,y)).repeat(epochs).\\\n",
        "              shuffle(buffer_size=10*batchsize).\\\n",
        "              map(augtest, num_parallel_calls=tf.data.experimental.AUTOTUNE).\\\n",
        "              batch(batchsize).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGuj-RQUUReo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        "\n",
        "\n",
        "def scheduler(epoch):\n",
        "  lr_schedule = PiecewiseLinear([0, 15, 20, 50], [0.0001, 0.004, 0.001, 0.0001])\n",
        "  return lr_schedule(epoch)\n",
        "\n",
        "def schedulert(epoch):\n",
        "  baselr = 0.004\n",
        "  if epoch < 12:\n",
        "    return baselr\n",
        "  else:\n",
        "    return max(baselr * tf.math.exp(0.05 * (12 - epoch)), 0.00001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2Rpzm6mOr0M",
        "colab": {}
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # idx = np.arange(trainX.shape[0])\n",
        "  # np.random.shuffle(idx)\n",
        "  # splitperc = 1.0\n",
        "  # trainValSplitIdx = int(splitperc*trainX.shape[0])\n",
        "  batchsize = 256\n",
        "  epochs = 60\n",
        "  traingen = tfdatagen(trainX,trainY,batchsize=batchsize, epochs=epochs,\\\n",
        "                       train=True)\n",
        "  #valgen = tfdatagen(trainX[idx[trainValSplitIdx:]], \\\n",
        "  #                   trainY[idx[trainValSplitIdx:]], \\\n",
        "  #                   batchsize=batchsize, epochs=epochs)\n",
        "  testgen = tfdatagen(testX, testY, batchsize=batchsize, epochs=epochs,\\\n",
        "                      train=False)\n",
        "\n",
        "  model = define_model()\n",
        "  print(model.summary())\n",
        "  opt = Adam(lr=0.004)\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "  model.compile(optimizer=opt, loss=loss,\\\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  steps = int(trainX.shape[0] / batchsize)\n",
        "  #steps_valid = int((1-splitperc)*trainX.shape[0] / batchsize)\n",
        "  steps_test = int(testX.shape[0]/batchsize)\n",
        "\n",
        "  callback = tf.keras.callbacks.LearningRateScheduler(schedulert, verbose=1)\n",
        "  #callback = []\n",
        "  history = model.fit(traingen, steps_per_epoch=steps, epochs=epochs,\\\n",
        "                      callbacks=[callback], validation_data=testgen, \\\n",
        "                      validation_steps=steps_test, verbose=1)\n",
        "  \n",
        "  _, acc = model.evaluate(testgen, verbose=1)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "  summarize_diagnostics(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7FgNnOpJkru",
        "colab_type": "code",
        "outputId": "a0131b3d-b422-43c6-d08f-306fa4ecceac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "  # entry point, run the test harness\n",
        "  run_test_harness()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_156 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_156 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_157 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_157 (Bat (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_78 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_158 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_158 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_159 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_159 (Bat (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_79 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_160 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_160 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_161 (Conv2D)          (None, 8, 8, 192)         221376    \n",
            "_________________________________________________________________\n",
            "batch_normalization_161 (Bat (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_80 (MaxPooling (None, 4, 4, 192)         0         \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 4, 4, 192)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_26  (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                1930      \n",
            "=================================================================\n",
            "Total params: 503,498\n",
            "Trainable params: 502,282\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train for 195 steps, validate for 39 steps\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 1/60\n",
            "195/195 [==============================] - 13s 67ms/step - loss: 1.9183 - accuracy: 0.4353 - val_loss: 1.4033 - val_accuracy: 0.5014\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 2/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.6173 - accuracy: 0.6114 - val_loss: 1.0964 - val_accuracy: 0.6480\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 3/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.5024 - accuracy: 0.6863 - val_loss: 0.8888 - val_accuracy: 0.7296\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 4/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.4290 - accuracy: 0.7333 - val_loss: 0.7558 - val_accuracy: 0.7689\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 5/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.3863 - accuracy: 0.7589 - val_loss: 0.7376 - val_accuracy: 0.7788\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 6/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.3567 - accuracy: 0.7785 - val_loss: 0.6142 - val_accuracy: 0.8185\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 7/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.3314 - accuracy: 0.7920 - val_loss: 0.6507 - val_accuracy: 0.7961\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 8/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.3152 - accuracy: 0.8013 - val_loss: 0.6140 - val_accuracy: 0.8196\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 9/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2984 - accuracy: 0.8109 - val_loss: 0.5913 - val_accuracy: 0.8268\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 10/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2876 - accuracy: 0.8155 - val_loss: 0.5516 - val_accuracy: 0.8339\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 11/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2766 - accuracy: 0.8221 - val_loss: 0.5439 - val_accuracy: 0.8442\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.004.\n",
            "Epoch 12/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2640 - accuracy: 0.8311 - val_loss: 0.5135 - val_accuracy: 0.8519\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.004, shape=(), dtype=float32).\n",
            "Epoch 13/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2558 - accuracy: 0.8350 - val_loss: 0.5332 - val_accuracy: 0.8454\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0038049177, shape=(), dtype=float32).\n",
            "Epoch 14/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2470 - accuracy: 0.8404 - val_loss: 0.5097 - val_accuracy: 0.8544\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0036193496, shape=(), dtype=float32).\n",
            "Epoch 15/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2358 - accuracy: 0.8471 - val_loss: 0.5037 - val_accuracy: 0.8603\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0034428318, shape=(), dtype=float32).\n",
            "Epoch 16/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2299 - accuracy: 0.8499 - val_loss: 0.5059 - val_accuracy: 0.8655\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0032749232, shape=(), dtype=float32).\n",
            "Epoch 17/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2192 - accuracy: 0.8561 - val_loss: 0.4909 - val_accuracy: 0.8637\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.0031152032, shape=(), dtype=float32).\n",
            "Epoch 18/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2169 - accuracy: 0.8577 - val_loss: 0.5007 - val_accuracy: 0.8593\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.0029632726, shape=(), dtype=float32).\n",
            "Epoch 19/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2081 - accuracy: 0.8632 - val_loss: 0.4938 - val_accuracy: 0.8605\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.0028187523, shape=(), dtype=float32).\n",
            "Epoch 20/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.2042 - accuracy: 0.8645 - val_loss: 0.4558 - val_accuracy: 0.8700\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.0026812803, shape=(), dtype=float32).\n",
            "Epoch 21/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.1985 - accuracy: 0.8680 - val_loss: 0.4632 - val_accuracy: 0.8725\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025505128, shape=(), dtype=float32).\n",
            "Epoch 22/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.1947 - accuracy: 0.8716 - val_loss: 0.4453 - val_accuracy: 0.8786\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0024261228, shape=(), dtype=float32).\n",
            "Epoch 23/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.1891 - accuracy: 0.8751 - val_loss: 0.4723 - val_accuracy: 0.8728\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0023077992, shape=(), dtype=float32).\n",
            "Epoch 24/60\n",
            "195/195 [==============================] - 11s 56ms/step - loss: 1.1854 - accuracy: 0.8757 - val_loss: 0.4422 - val_accuracy: 0.8797\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0021952465, shape=(), dtype=float32).\n",
            "Epoch 25/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1824 - accuracy: 0.8778 - val_loss: 0.4570 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.0020881833, shape=(), dtype=float32).\n",
            "Epoch 26/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1760 - accuracy: 0.8817 - val_loss: 0.4267 - val_accuracy: 0.8859\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.0019863413, shape=(), dtype=float32).\n",
            "Epoch 27/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1729 - accuracy: 0.8836 - val_loss: 0.4492 - val_accuracy: 0.8778\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0018894663, shape=(), dtype=float32).\n",
            "Epoch 28/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1706 - accuracy: 0.8842 - val_loss: 0.4327 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.0017973159, shape=(), dtype=float32).\n",
            "Epoch 29/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1649 - accuracy: 0.8883 - val_loss: 0.4363 - val_accuracy: 0.8868\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0017096598, shape=(), dtype=float32).\n",
            "Epoch 30/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1627 - accuracy: 0.8889 - val_loss: 0.4360 - val_accuracy: 0.8834\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.0016262789, shape=(), dtype=float32).\n",
            "Epoch 31/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1633 - accuracy: 0.8879 - val_loss: 0.4212 - val_accuracy: 0.8903\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0015469643, shape=(), dtype=float32).\n",
            "Epoch 32/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1560 - accuracy: 0.8928 - val_loss: 0.4075 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.0014715178, shape=(), dtype=float32).\n",
            "Epoch 33/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1521 - accuracy: 0.8961 - val_loss: 0.4267 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.0013997512, shape=(), dtype=float32).\n",
            "Epoch 34/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1514 - accuracy: 0.8948 - val_loss: 0.4343 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.0013314844, shape=(), dtype=float32).\n",
            "Epoch 35/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1484 - accuracy: 0.8968 - val_loss: 0.4243 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0012665471, shape=(), dtype=float32).\n",
            "Epoch 36/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1466 - accuracy: 0.8984 - val_loss: 0.4135 - val_accuracy: 0.8937\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.0012047768, shape=(), dtype=float32).\n",
            "Epoch 37/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1462 - accuracy: 0.8989 - val_loss: 0.4196 - val_accuracy: 0.8911\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0011460192, shape=(), dtype=float32).\n",
            "Epoch 38/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1434 - accuracy: 0.8999 - val_loss: 0.4092 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0010901273, shape=(), dtype=float32).\n",
            "Epoch 39/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1409 - accuracy: 0.9011 - val_loss: 0.4085 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.001036961, shape=(), dtype=float32).\n",
            "Epoch 40/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1375 - accuracy: 0.9031 - val_loss: 0.4032 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.000986388, shape=(), dtype=float32).\n",
            "Epoch 41/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1385 - accuracy: 0.9031 - val_loss: 0.4160 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009382811, shape=(), dtype=float32).\n",
            "Epoch 42/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1371 - accuracy: 0.9029 - val_loss: 0.4100 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008925207, shape=(), dtype=float32).\n",
            "Epoch 43/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1347 - accuracy: 0.9063 - val_loss: 0.4279 - val_accuracy: 0.8929\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.000848992, shape=(), dtype=float32).\n",
            "Epoch 44/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1326 - accuracy: 0.9065 - val_loss: 0.4145 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080758607, shape=(), dtype=float32).\n",
            "Epoch 45/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1304 - accuracy: 0.9069 - val_loss: 0.4127 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00076819974, shape=(), dtype=float32).\n",
            "Epoch 46/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1284 - accuracy: 0.9087 - val_loss: 0.4152 - val_accuracy: 0.8933\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007307341, shape=(), dtype=float32).\n",
            "Epoch 47/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1275 - accuracy: 0.9096 - val_loss: 0.4089 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069509575, shape=(), dtype=float32).\n",
            "Epoch 48/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1255 - accuracy: 0.9106 - val_loss: 0.4067 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006611956, shape=(), dtype=float32).\n",
            "Epoch 49/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1241 - accuracy: 0.9119 - val_loss: 0.4018 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006289487, shape=(), dtype=float32).\n",
            "Epoch 50/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1245 - accuracy: 0.9098 - val_loss: 0.4071 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005982745, shape=(), dtype=float32).\n",
            "Epoch 51/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1234 - accuracy: 0.9116 - val_loss: 0.4034 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005690963, shape=(), dtype=float32).\n",
            "Epoch 52/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1226 - accuracy: 0.9125 - val_loss: 0.4103 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054134114, shape=(), dtype=float32).\n",
            "Epoch 53/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1203 - accuracy: 0.9150 - val_loss: 0.4085 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005149396, shape=(), dtype=float32).\n",
            "Epoch 54/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1210 - accuracy: 0.9128 - val_loss: 0.3973 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004898258, shape=(), dtype=float32).\n",
            "Epoch 55/60\n",
            "195/195 [==============================] - 11s 58ms/step - loss: 1.1182 - accuracy: 0.9156 - val_loss: 0.4049 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004659366, shape=(), dtype=float32).\n",
            "Epoch 56/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1167 - accuracy: 0.9151 - val_loss: 0.4079 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044321263, shape=(), dtype=float32).\n",
            "Epoch 57/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1169 - accuracy: 0.9155 - val_loss: 0.3988 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004215969, shape=(), dtype=float32).\n",
            "Epoch 58/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1163 - accuracy: 0.9161 - val_loss: 0.4004 - val_accuracy: 0.9020\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040103542, shape=(), dtype=float32).\n",
            "Epoch 59/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1187 - accuracy: 0.9152 - val_loss: 0.3976 - val_accuracy: 0.9002\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038147668, shape=(), dtype=float32).\n",
            "Epoch 60/60\n",
            "195/195 [==============================] - 11s 57ms/step - loss: 1.1136 - accuracy: 0.9181 - val_loss: 0.4096 - val_accuracy: 0.8964\n",
            "2344/2344 [==============================] - 34s 14ms/step - loss: 0.4085 - accuracy: 0.8968\n",
            "> 89.680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH1pVevLdJcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}